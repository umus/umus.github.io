\documentclass{article}

\usepackage{ams-mdbch}
\usepackage{amsmath}


\DeclareMathOperator{\Span}{Span}

\begin{document}
\section{Definitions}
\subsection*{Chapter 5}
\subsubsection*{5.1}

A linear operator $T$ on a finite dimensional vector space $V$ is called \textit{diagonalizable} if there exists an ordered basis $\beta$ of $V$ such that $\left[T\right]_{\beta}$ is diagonal. A square matrix $A$ is called diagonalizable if $L_A$ is diagonalizable. \\

Given a linear operator $T$ on a vector space $V$, an \textit{eigenvector} is a non-zero vector $x$ such that $T(x) = \lambda x$, for some $\lambda \in F$. This $\lambda$ is called the \textit{eigenvalue} corresponding to eigenvector $x$. \\

For a square matrix $A$, the \textit{characteristic polynomial} of $A$ is the polynomial $f(t) = \det( A - t I)$. The characteristic polynomial of a linear operator $T$ is the polynomial $f(t) = \det( [T]_\beta - t I)$, for any basis $\beta$ of $V$. \\

\subsubsection*{5.2}

A polynomail $f(t)$ \textit{splits} over a field $F$ if it can be expressed as a product of linear terms. That is, $f(t) = \prod_i (\lambda_i - t)$ for $\lambda_i \in F$. \\

The \textit{algebraic multiplicity} of a root $\lambda$ of a polynomial $f(t)$ is the greatest $m$ such that $(t-\lambda)^m$ divides $f(t)$. \\

The \textit{eigenspace} of an eigenvalue $\lambda$ for a linear operator $T: V \rightarrow V$ is the subspace $E_\lambda = \{x: T(x) = \lambda x\}$. \\


The \textit{geometric multiplicity} of an eigenvalue $\lambda$ is the dimension of $E_\lambda$. \\

Given a vector space $V$ and an arbitrary family of subspaces $(W_{\alpha})_{\alpha \in A}$, the \textit{sum} of subspaces, denoted by $\sum_{\alpha \in A} W_\alpha$ is the set of vectors $\left\{\sum_{\alpha \in A} x_\alpha: x_\alpha \in W_\alpha\right\}$. \\

The sum $\sum_{\alpha \in A} W_\alpha$ is called \textit{direct} if it decomposes the vector space $V$ uniquely. Explicitly, for an arbitrary $x \in V$, if
$x = \sum_{\alpha \in A} \zeta_\alpha w_\alpha$, then the scalars $\zeta_\alpha$ are unique. \\

\subsubsection*{5.3}

For a linear operator $T$ on a vector space $V$, a subsapce $W$ is called \textit{T-invariant} if $T(W)=W$. In other words,  $x \in W \Rightarrow T(x) \in W$. \\

For a vector $x \in V$, The \textit{T-cyclic subspace generated by v} is the subspace given by $\Span\{x, T(x), T^2(x), \ldots \}$. \\

For a linear operator $T$ on a vector space $V$ and some $T$-invariant subspace $W$, define $\bar{T}: V/W \rightarrow V/W$ by $x+W \mapsto T(x)+W$ for $x \in V$.  

\subsection*{Chapter 6}
\subsection*{6.1}

Given a vector space $V$ over field $F$, we define an \textit{inner product} to be a function $\langle x, y\rangle: V \times V \mapsto F$  field which is 
\begin{enumerate}
\item Linear in the first component,
\item Symmetric under complex conjugation,
\item Positive definite.
\end{enumerate} \\

Note that conjugate linearity in the second component follows immediately from these properties. 

If $F = \mathbb{R}$ or $F= \mathbb{C}$ and the $V =  F^n$, we call the inner product $\langle x, y \rangle = \sum_i x_i\overline{y_i}$ the \textit{standard inner product} on $F^n$. \\

For a square matrix $A$, the \textit{conjugate transpose} of $A$, denoted $A^*$, is the matrix given by $A^*_{ij} = \overline{A_{ji}}$. Note that if $F = \mathbb{R}$, $A^* = A^t$. \\

A vector space $V$ over $F$, endowed with a specific inner product is called an \textit{inner product space}. Naturally, if $F = \mathbb{R}$, it is a \textit{real inner product space} and if $F= \mathbb{C}$ it is a \textit{complex inner product space}. \\

The \textit{length} of a vector $v$ in an inner product space $V$, denoted by $||x||$, is given by $||x|| = \sqrt{\langle x, x \rangle }$. \\

Two vectors in an inner product space $V$ are called \textit{orthogonal} if $\langle x, y \rangle = 0$. A subset $S \subset V$ is called orthogonal if $\langle x, y \rangle = 0$ for all distinct $x, y \in S$. A \textit{unit vector} is a vector with length one. An \textit{orthonormal} subset $S$ is an orthogonal set of unit vectors. Equivalently, $S$ is orthonormal if $\langle x, y \rangle = \delta_{xy}$. \\

\subsubsection*{6.2}

An \textit{orthonormal basis} is a basis of a inner product space $V$ which is orthonormal. \\

Given a subset $S$ of an inner product space $V$, we obtain a natural subspace called the \textit{orthogonal complement} of $S$, denoted by $S^\perp$, which is the set $S^\perp = \left\{x \in V: \langle x, y \rangle = 0, \ \forall y \in S \right\}$. If $V$ is finite dimensionl, and $W$ a subspace, the sum $W + W^\perp$ is direct. \\

For a vector $x$ in an inner product space We defined the \textit{orthogonal projection} of $x$ onto $W$ by $x \mapsto y $, where $x = y + z$ for $y \in W^\perp$ and $z \in W$ 
Moreover, $y = x_{W^\perp}$ is the unique vector in $W^\perp$ such that $x - y \in W$. \\

\subsubsection*{6.3}

For a finite dimensional inner product space $V$, the \textit{adjoint} of a linear operator $T: V \to V$ is the unique linear operator $T^*: V \to V$ such that $\langle T(x), y \rangle = \langle x, T^*(y) \rangle$ for all $x, y \in V$. 

\subsubsection*{6.4}

For a finite dimensional inner product space $V$, a linear operator $T$ is called \textit{normal} if it commutes with its adjoint. That is, $TT^* = T^*T$. A matrix $A$ is normal if $AA^* = A^*A$. \\

With $V$ as before, a linear operator $T$ is called \textit{self-adjoint} if it is its own adjoint: $T = T^*$. A matrix $A$ is called self-adjoint if $A=A*$. \textit{Hermitian} is a synonym for self-adjoint. \\

A square matrix $A$ with $F = \mathbb{R}$ is called \textit{Gramian} if there exists a real matrix $B$ such that $A = B^tB$. \\

With $V$ as before, a linear operator $T$ is called \textit{positive definite} if $T$ is self-adjoint and $\langle T(x), x \rangle > 0$ for all $x \neq 0$. A linear operator $T$ is called \textit{semi-positive definite} if $T$ is self-adjoint and $\langle T(x), x \rangle \geq 0$ for all $x \neq 0$. The definitions for matrices are analagous in the obvious way. 

\section{Theorems}

\subsection*{Chapter 5}



\subsubsection*{5.1}

\textbf{Theorem 5.2} 
A scalar $\lambda$ is an eigenvalue of a square matrix $A$ if and only if $\det(A-\lambda I) = 0$. \\

\subsubsection*{5.2}

\textbf{Theorem 5.5}
For a linear operator $T$ over $n$-dimensional $V$ with distinct eigenvalues $\lambda_i$,  for $v_i$ an eigenvector corresponding to $\lambda_i$, the set $\{v_i\}$ is linearly independent. Consequently, if $T$ has $n$ distinct eigenvalues, it is diagonalizable. \\
\textbf{Theorem 5.9}
A linear operator $T$ over $V$ with a splitting characteristic polynomial is diagonalizable if and only if its geometric and algebraic multiplicities are equal. Furthermore, the union of the bases of the eigenspaces form a basis for $V$. \\
\textbf{Theorem 5.11} A linear operator $T$ on a finite dimensionl vector space $V$ is diagonalizabl if and only if its eigenspaces form a direct decomposition.\\

\subsubsection*{5.4}
\textbf{Theorem 5.22} If $T$ is a linear operator over a $k$-dimensional vector space $V$, and $W$ is the cyclic subspace generate by a non-zero vector $x$, then the set $\{x, T(x), \ldots T^{k-1}(x)\}$ is a basis for $W$ and the scalars $\zeta_i$ in the linear comibnation $\sum_i \zeta_i T^i(x) = - T^k(x)$ give the characteristic polynomial of $T_w$ by $f(t) = (-1)^k \sum_i a_i t^i$ \\
\textbf{Cayley-Hamilton Theorem}
 A linear operator on a finite dimensional vector space satisfies its characteristic polynomial. The same holds for square matrices.\\ 
\subsection*{Chapter 6} 
\subsubsection*{6.1}
\textbf{Theorem 6.1 (e)} If $\langle x, y \rangle = \langle x, z \rangle $ for all $x \in V$, then $y = z$. \\
\textbf{Cauchy-Schwarz Inequality} $|\langle x, y \rangle| \leq ||x|| \cdot ||y||$ with equality when $x = \lambda y$. 
\textbf{Triangle Inequality} $||x+y||\leq ||x|| + ||y||$ with equality when $x = \lambda y$. 
\subsubsection*{6.2}
\textbf{Theorem 6.3} If $V$ is an inner product space and $S= \{ x_i: i = 1, \ldots, k\}$ is an orthogonal subset such that $v_i \neq 0$, then, if $y \in \Span S$, then $$y = \sum_{i=1}^k\frac{\langle y, x_i\rangle}{\langle x_i, x_i \rangle}v_i$$
\textbf{Gram-Schmidt Orthogonalization Process} With $V$ and $S$ as above, if we define $S' = \{ v_i: 1, \ldots, k\}$ by $v_1 = x_1$, and otherwise by 

$$v_i = x_i - \sum_{j = 1}^{k-1} \frac{\langle x_i, v_j \rangle}{\langle v_j, v_j\rangle} v_j$$
Then $S'$ is orthogonal and $\Span S = \Span S'$. \\ 
\textbf{Theorem 6.6} If $W$ is a finite dimensional subspace of an inner product space $V$, then $x = V$ can uniquly be expressed as a sum of vectors from $W$ and $W^\perp$. If $V$ is finite dimensional, $$ W \oplus W^\perp = V$$
\subsubsection*{6.3}
\textbf{Theorem 6.8} Every linear functional $g:V \to F$ is some inner product $g(x) = \langle x, y\rangle$ for a fixed $y \in V$. \\
\textbf{Theorem 6.9} If $\dim V < \infty$, Given $T: V \to V$, there exists a linear function $T^*: V \to V$ such that $\langle T(x) , y \rangle = \langle x, T^*(y) \rangle $ for all $x, y \in V$. \\
\textbf{Theorem 6.10} If $\beta$ is an orthonormal basis of $V$, then $[T^*]_\beta = ([T]_\beta)^*$. 


\subsubsection*{6.4}
\textbf{Lemma} If $T$, a linear operator on a finite-dimensional inner product space, has an eigenvector, then so does $T^*$. \\
\textbf{Theorem 6.14 (Schur)} With $T$ as above, if the characteristic polynomial of $T$ splits then there is an orthonormal basis $\beta$ of $V$ such that $[T]_\beta$ is upper triangular. \\
\textbf{Spectral Theorem(s)} If $T$ is a linear operator on a finite-dimensional vector space $V$ over field $F$, if $F= \mathbb{R}$ and $T$ is self adjoint OR if $f = \mathbb{C}$ and $T$ is normal, then $V$ has an orthonormal basis of eigenvectors. Furthermore, the converse is also true, in both cases. \\





% Note that $[T^*]_\beta = [T_\beta]^*$ for any orthonormal basis $\beta$. 

\end{document}
